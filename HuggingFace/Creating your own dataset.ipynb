{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6629',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6629/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6629/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6629/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6629',\n",
       "  'id': 2105774482,\n",
       "  'node_id': 'PR_kwDODunzps5lV0aF',\n",
       "  'number': 6629,\n",
       "  'title': 'Support push_to_hub without org/user to default to logged-in user',\n",
       "  'user': {'login': 'albertvillanova',\n",
       "   'id': 8515462,\n",
       "   'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/albertvillanova',\n",
       "   'html_url': 'https://github.com/albertvillanova',\n",
       "   'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "   'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "   'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 1,\n",
       "  'created_at': '2024-01-29T15:36:52Z',\n",
       "  'updated_at': '2024-01-29T15:41:30Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'MEMBER',\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6629',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/pull/6629',\n",
       "   'diff_url': 'https://github.com/huggingface/datasets/pull/6629.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/datasets/pull/6629.patch',\n",
       "   'merged_at': None},\n",
       "  'body': 'This behavior is aligned with:\\r\\n- the behavior of `datasets` before merging #6519\\r\\n  - the behavior described in the corresponding docstring\\r\\n- the behavior of `huggingface_hub.create_repo`\\r\\n\\r\\nRevert \"Support push_to_hub canonical datasets (#6519)\"\\r\\n- This reverts commit a887ee78835573f5d80f9e414e8443b4caff3541.\\r\\n\\r\\nFix #6597.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6629/reactions',\n",
       "   'total_count': 1,\n",
       "   '+1': 1,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6629/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GITHUB_TOKEN = \"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\" #\"Creating your own dataset.ipynb\"\n",
    "GITHUB_TOKEN = \"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\"\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import math\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# def fetch_issues(\n",
    "#     owner=\"huggingface\",\n",
    "#     repo=\"datasets\",\n",
    "#     num_issues=10_000,\n",
    "#     rate_limit=5_000,\n",
    "#     issues_path=Path(\".\"),\n",
    "# ):\n",
    "#     if not issues_path.is_dir():\n",
    "#         issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "#     batch = []\n",
    "#     all_issues = []\n",
    "#     per_page = 100  # Number of issues to return per page\n",
    "#     num_pages = math.ceil(num_issues / per_page)\n",
    "#     base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "#     for page in tqdm(range(num_pages)):\n",
    "#         # Query with state=all to get both open and closed issues\n",
    "#         query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "#         issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "#         batch.extend(issues.json())\n",
    "\n",
    "#         if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "#             all_issues.extend(batch)\n",
    "#             batch = []  # Flush batch for next time period\n",
    "#             print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "#             time.sleep(60 * 60 + 1)\n",
    "\n",
    "#     all_issues.extend(batch)\n",
    "#     df = pd.DataFrame.from_records(all_issues)\n",
    "#     df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "#     print(\n",
    "#         f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6323f442904da8aa51987f1b521063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n",
      "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/3164',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/3164/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/3164/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/3164/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/issues/3164',\n",
       " 'id': 1035662830,\n",
       " 'node_id': 'I_kwDODunzps49uvXu',\n",
       " 'number': 3164,\n",
       " 'title': 'Add raw data files to the Hub with GitHub LFS for canonical dataset',\n",
       " 'user': {'avatar_url': 'https://avatars.githubusercontent.com/u/40370937?v=4',\n",
       "  'events_url': 'https://api.github.com/users/zlucia/events{/privacy}',\n",
       "  'followers_url': 'https://api.github.com/users/zlucia/followers',\n",
       "  'following_url': 'https://api.github.com/users/zlucia/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/zlucia/gists{/gist_id}',\n",
       "  'gravatar_id': '',\n",
       "  'html_url': 'https://github.com/zlucia',\n",
       "  'id': 40370937,\n",
       "  'login': 'zlucia',\n",
       "  'node_id': 'MDQ6VXNlcjQwMzcwOTM3',\n",
       "  'organizations_url': 'https://api.github.com/users/zlucia/orgs',\n",
       "  'received_events_url': 'https://api.github.com/users/zlucia/received_events',\n",
       "  'repos_url': 'https://api.github.com/users/zlucia/repos',\n",
       "  'site_admin': False,\n",
       "  'starred_url': 'https://api.github.com/users/zlucia/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/zlucia/subscriptions',\n",
       "  'type': 'User',\n",
       "  'url': 'https://api.github.com/users/zlucia'},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': {'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "  'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "  'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "  'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "  'gravatar_id': '',\n",
       "  'html_url': 'https://github.com/albertvillanova',\n",
       "  'id': 8515462,\n",
       "  'login': 'albertvillanova',\n",
       "  'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "  'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "  'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "  'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "  'site_admin': False,\n",
       "  'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "  'type': 'User',\n",
       "  'url': 'https://api.github.com/users/albertvillanova'},\n",
       " 'assignees': [{'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "   'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "   'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "   'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "   'gravatar_id': '',\n",
       "   'html_url': 'https://github.com/albertvillanova',\n",
       "   'id': 8515462,\n",
       "   'login': 'albertvillanova',\n",
       "   'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "   'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "   'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "   'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "   'site_admin': False,\n",
       "   'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "   'type': 'User',\n",
       "   'url': 'https://api.github.com/users/albertvillanova'}],\n",
       " 'milestone': None,\n",
       " 'comments': 3,\n",
       " 'created_at': Timestamp('2021-10-25 23:28:21+0000', tz='UTC'),\n",
       " 'updated_at': Timestamp('2021-10-30 19:54:51+0000', tz='UTC'),\n",
       " 'closed_at': Timestamp('2021-10-30 19:54:51+0000', tz='UTC'),\n",
       " 'author_association': 'NONE',\n",
       " 'active_lock_reason': None,\n",
       " 'draft': None,\n",
       " 'pull_request': None,\n",
       " 'body': \"I'm interested in sharing the CaseHOLD dataset (https://arxiv.org/abs/2104.08671) as a canonical dataset on the HuggingFace Hub and would like to add the raw data files to the Hub with GitHub LFS, since it seems like a more sustainable long term storage solution, compared to other storage solutions available to my team. From what I can tell, this option is not immediately supported if one follows the sharing steps detailed here: [https://huggingface.co/docs/datasets/share_dataset.html#sharing-a-canonical-dataset](https://huggingface.co/docs/datasets/share_dataset.html#sharing-a-canonical-dataset), since GitHub LFS is not supported for public forks. Is there a way to request this? Thanks!\",\n",
       " 'reactions': {'+1': 0,\n",
       "  '-1': 0,\n",
       "  'confused': 0,\n",
       "  'eyes': 0,\n",
       "  'heart': 0,\n",
       "  'hooray': 0,\n",
       "  'laugh': 0,\n",
       "  'rocket': 0,\n",
       "  'total_count': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/datasets/issues/3164/reactions'},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/3164/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': 'completed',\n",
       " '__index_level_0__': 3483}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "issues_dataset\n",
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url',\n",
       " 'repository_url',\n",
       " 'labels_url',\n",
       " 'comments_url',\n",
       " 'events_url',\n",
       " 'html_url',\n",
       " 'id',\n",
       " 'node_id',\n",
       " 'number',\n",
       " 'title',\n",
       " 'user',\n",
       " 'labels',\n",
       " 'state',\n",
       " 'locked',\n",
       " 'assignee',\n",
       " 'assignees',\n",
       " 'milestone',\n",
       " 'comments',\n",
       " 'created_at',\n",
       " 'updated_at',\n",
       " 'closed_at',\n",
       " 'author_association',\n",
       " 'active_lock_reason',\n",
       " 'draft',\n",
       " 'pull_request',\n",
       " 'body',\n",
       " 'reactions',\n",
       " 'timeline_url',\n",
       " 'performed_via_github_app',\n",
       " 'state_reason',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.github.com/repos/huggingface/datasets/issues/6629'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b846acc82b4470875d31ec3dab6569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m refined_issues_dataset \u001b[38;5;241m=\u001b[39m \u001b[43missues_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_pull_request\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpull_request\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m else:\n\u001b[0;32m    591\u001b[0m     self: \"Dataset\" = kwargs.pop(\"self\")\n\u001b[1;32m--> 592\u001b[0m # apply actual function\n\u001b[0;32m    593\u001b[0m out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n\u001b[0;32m    594\u001b[0m datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m unformatted_columns = set(self.column_names) - set(self._format_columns or [])\n\u001b[0;32m    551\u001b[0m self_format = {\n\u001b[0;32m    552\u001b[0m     \"type\": self._format_type,\n\u001b[0;32m    553\u001b[0m     \"format_kwargs\": self._format_kwargs,\n\u001b[0;32m    554\u001b[0m     \"columns\": self._format_columns,\n\u001b[0;32m    555\u001b[0m     \"output_all_columns\": self._output_all_columns,\n\u001b[0;32m    556\u001b[0m }\n\u001b[1;32m--> 557\u001b[0m # apply actual function\n\u001b[0;32m    558\u001b[0m out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n\u001b[0;32m    559\u001b[0m datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:3093\u001b[0m, in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3091\u001b[0m shards_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 3093\u001b[0m     transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3094\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m         transformed_dataset \u001b[38;5;241m=\u001b[39m load_processed_shard_from_cache(dataset_kwargs)\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:3446\u001b[0m, in \u001b[0;36m_map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3443\u001b[0m try:\n\u001b[0;32m   3444\u001b[0m     arrow_formatted_shard = shard.with_format(\"arrow\")\n\u001b[1;32m-> 3446\u001b[0m     # Loop over single examples or batches and write to buffer/file if examples are to be updated\n\u001b[0;32m   3447\u001b[0m     if not batched:\n\u001b[0;32m   3448\u001b[0m         shard_iterable = enumerate(arrow_formatted_shard)\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:3349\u001b[0m, in \u001b[0;36mapply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3343\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m update_data\n\u001b[0;32m   3345\u001b[0m inputs \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   3346\u001b[0m     pa_inputs,\n\u001b[0;32m   3347\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_inputs\u001b[38;5;241m.\u001b[39mnum_rows),\n\u001b[0;32m   3348\u001b[0m     format_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m-> 3349\u001b[0m     formatter\u001b[38;5;241m=\u001b[39minput_formatter,\n\u001b[0;32m   3350\u001b[0m )\n\u001b[0;32m   3351\u001b[0m fn_args \u001b[38;5;241m=\u001b[39m [inputs] \u001b[38;5;28;01mif\u001b[39;00m input_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [inputs[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m input_columns]\n\u001b[0;32m   3352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m refined_issues_dataset \u001b[38;5;241m=\u001b[39m issues_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_pull_request\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;129;01mor\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpull_request\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "refined_issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_pull_request\" : False if \n",
    "               try:\n",
    "                x['pull_request'] in None\n",
    "               except :\n",
    "                    pass\n",
    "               else True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column url not in the dataset. Current columns in the dataset: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     url, pr \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpull_request\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2799\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:2784\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2786\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\formatting\\formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 580\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\formatting\\formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[1;34m(key, columns)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column url not in the dataset. Current columns in the dataset: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16']\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     url, pr \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m], sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpull_request\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     url, pr \u001b[38;5;241m=\u001b[39m  \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>> URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\arrow_dataset.py:2784\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2782\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2786\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2787\u001b[0m )\n\u001b[0;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\formatting\\formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 580\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[1;32md:\\DevLearn\\PedagogyPanda\\env\\.hf\\Lib\\site-packages\\datasets\\formatting\\formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[1;34m(key, columns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column url not in the dataset. Current columns in the dataset: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16']\""
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for  issue in sample:\n",
    "    try:\n",
    "        url, pr = sample[\"url\"], sample[\"pull_request\"]\n",
    "    except:\n",
    "        url, pr =  sample[\"url\"], None\n",
    "    finally:\n",
    "        print(f\">> URL: {url}\")\n",
    "        # print(f\">> Pull request: {pr}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
